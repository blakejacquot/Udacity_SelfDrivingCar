{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.0\n"
     ]
    }
   ],
   "source": [
    "import jupyter\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import sklearn\n",
    "import tensorflow\n",
    "print(cv2.__version__)\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jupyter\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import sklearn\n",
    "import tensorflow\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "\"\"\"Banging the data into usable format\"\"\"\n",
    "def randomize_set(x,y):\n",
    "    numel = len(y)\n",
    "    print(type(x), type(y))\n",
    "    print(x.shape, y.shape)\n",
    "    listicle = [[i] for i in range(numel)]\n",
    "    random.shuffle(listicle)\n",
    "    x_shape = x.shape\n",
    "    y_shape = y.shape\n",
    "    ret_x = np.ones((x_shape[0],x_shape[1],x_shape[2]))\n",
    "    ret_y = np.ones((x_shape[0]))\n",
    "    print(ret_x.shape, ret_y.shape)\n",
    "    for i in range(numel):\n",
    "        index = listicle[i]\n",
    "        curr_x = x[index,:,:]\n",
    "        curr_y = y[index]\n",
    "        ret_x[i,:,:] = curr_x\n",
    "        ret_y[i] = curr_y\n",
    "        #print(index)\n",
    "    return(ret_x,ret_y)\n",
    "\n",
    "def make_one_hot_encoding(y, num_labels):\n",
    "    print('Making one hot encoding')\n",
    "    print(type(y), type(num_labels))\n",
    "    print(y.shape, num_labels)\n",
    "    y_shape = y.shape\n",
    "    numel = y_shape[0]\n",
    "    print(numel)\n",
    "    #for i in range(numel):\n",
    "    ret_y = np.zeros((numel, num_labels))\n",
    "    print('Return y = ', ret_y.shape)\n",
    "    for i in range(numel):\n",
    "        curr_label = y[i]\n",
    "        #print(i, curr_label)\n",
    "        curr_encoding = np.zeros(num_labels)\n",
    "        for j in range(num_labels):\n",
    "            if j == int(curr_label):\n",
    "                #print('Match!', j, curr_label)\n",
    "                curr_encoding[j] = 1.0\n",
    "        #print(curr_encoding)\n",
    "        ret_y[i] = curr_encoding\n",
    "    return ret_y\n",
    "\n",
    "def expand_x(x):\n",
    "    shape_x = x.shape\n",
    "    print('Length is = ', len(shape_x))\n",
    "    if len(shape_x) == 3:\n",
    "        print('Expanding dataset to [num el, row, col, 1]')\n",
    "        ret_x = np.empty((shape_x[0],shape_x[1],shape_x[2],1))\n",
    "        ret_x[:,:,:,0] = x\n",
    "        #print(ret_x.shape)\n",
    "        #print('Example value = ', ret_x[0,0,0,0])\n",
    "    return(ret_x)\n",
    "\n",
    "\"\"\"Neural Network helper functions\"\"\"\n",
    "def conv2d(x, W, b, strides=1):\n",
    "\t\"\"\"\n",
    "\tArgs:\n",
    "\t\tx\n",
    "\t\tW\n",
    "\t\tb\n",
    "\t\tstrides\n",
    "\tReturns:\n",
    "\t\tTBD\n",
    "\t\"\"\"\n",
    "\tx = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "\tx = tf.nn.bias_add(x, b)\n",
    "\treturn tf.nn.tanh(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "\t\"\"\"\n",
    "\tArgs:\n",
    "\t\tx\n",
    "\t\tk\n",
    "\tReturns:\n",
    "\t\tTBD\n",
    "\t\"\"\"\n",
    "\treturn tf.nn.max_pool(\n",
    "\t    x,\n",
    "\t    ksize=[1, k, k, 1],\n",
    "\t    strides=[1, k, k, 1],\n",
    "\t    padding='SAME')\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases):\n",
    "\t\"\"\"\n",
    "\tArgs:\n",
    "  \t\tx:\n",
    "  \t\tweights:\n",
    "  \t\tbiases:\n",
    "\n",
    "\tReturns:\n",
    "\t\tout:\n",
    "\n",
    "\t\"\"\"\n",
    "#0 input 1 or 3 maps of 48x48 neurons\n",
    "#1 convolutional 100 maps of 46x46 neurons 3x3\n",
    "#2 max pooling 100 maps of 23x23 neurons 2x2\n",
    "#3 convolutional 150 maps of 20x20 neurons 4x4\n",
    "#4 max pooling 150 maps of 10x10 neurons 2x2\n",
    "#5 convolutional 250 maps of 8x8 neurons 3x3\n",
    "#6 max pooling 250 maps of 4x4 neurons 2x2\n",
    "#7 fully connected 200 neurons\n",
    "#8 fully connected 43 neurons\n",
    "\n",
    "\n",
    "\n",
    "#1 convolutional 100 maps of 46x46 neurons 3x3\n",
    "#2 max pooling 100 maps of 23x23 neurons 2x2\n",
    "    # Layer 1\n",
    "\tconv1 = conv2d(x, weights['layer_1'], biases['layer_1'])\n",
    "\tconv1 = maxpool2d(conv1)\n",
    "\n",
    "#3 convolutional 150 maps of 20x20 neurons 4x4\n",
    "#4 max pooling 150 maps of 10x10 neurons 2x2\n",
    "    # Layer 2\n",
    "\tconv2 = conv2d(conv1, weights['layer_2'], biases['layer_2'])\n",
    "\tconv2 = maxpool2d(conv2)\n",
    "\n",
    "#5 convolutional 250 maps of 8x8 neurons 3x3\n",
    "#6 max pooling 250 maps of 4x4 neurons 2x2\n",
    "    # Layer 3\n",
    "\tconv3 = conv2d(conv2, weights['layer_3'], biases['layer_3'])\n",
    "\tconv3 = maxpool2d(conv2)\n",
    "\n",
    "#7 fully connected 200 neurons\n",
    "#8 fully connected 43 neurons\n",
    "    # Fully connected layer\n",
    "    # Reshape conv3 output to fit fully connected layer input\n",
    "\tfc1 = tf.reshape(\n",
    "        conv3,\n",
    "        [-1, weights['fully_connected'].get_shape().as_list()[0]])\n",
    "\tfc1 = tf.add(\n",
    "        tf.matmul(fc1, weights['fully_connected']),\n",
    "        biases['fully_connected'])\n",
    "\tfc1 = tf.nn.tanh(fc1)\n",
    "\n",
    "    # Output Layer - class prediction\n",
    "\tout = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "\treturn out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\"\"\"Pre processing helper functions\"\"\"\n",
    "def make_gaussian_blur(x, kernel_size):\n",
    "    x_shape = x.shape\n",
    "    print(x_shape)\n",
    "    num_el = x_shape[0]\n",
    "    ret_images = np.ones((x_shape[0],x_shape[1],x_shape[2]))\n",
    "    print(ret_images.shape)\n",
    "    for i in range(num_el):\n",
    "        curr_im = x[i][:][:][:]\n",
    "        ret_images[i][:][:] = gaussian_blur(curr_im, kernel_size)\n",
    "    return ret_images\n",
    "\n",
    "def crop_to_ROI(x, vertices):\n",
    "    x_shape = x.shape\n",
    "    print(x_shape)\n",
    "    num_el = x_shape[0]\n",
    "    ret_images = np.ones((x_shape[0],x_shape[1],x_shape[2]))\n",
    "    print(ret_images.shape)\n",
    "    for i in range(num_el):\n",
    "        curr_im = x[i][:][:][:]\n",
    "        ret_images[i][:][:] = get_ROI(curr_im, vertices)\n",
    "    return ret_images\n",
    "\n",
    "def normalize(x):\n",
    "    x_shape = x.shape\n",
    "    print(x_shape)\n",
    "    num_el = x_shape[0]\n",
    "    ret_images = np.ones((x_shape[0],x_shape[1],x_shape[2]))\n",
    "    print(ret_images.shape)\n",
    "    for i in range(num_el):\n",
    "        curr_im = x[i][:][:][:]\n",
    "        empty_im = np.ones((x_shape[1],x_shape[2]))\n",
    "        #print('Normalizing image')\n",
    "        #print(np.ndarray.max(curr_im), np.ndarray.min(curr_im))\n",
    "        #proc_im = cv2.normalize(curr_im, empty_im, 0,255,cv2.NORM_MINMAX)\n",
    "        proc_im = cv2.normalize(curr_im, empty_im, -127,128,cv2.NORM_MINMAX)\n",
    "        #print(np.ndarray.max(proc_im), np.ndarray.min(proc_im))\n",
    "        ret_images[i][:][:] = proc_im\n",
    "    return ret_images\n",
    "\n",
    "\n",
    "\"\"\"Helper functions from Project 1\"\"\"\n",
    "def grayscale(img):\n",
    "    \"\"\"Applies the Grayscale transform\n",
    "    This will return an image with only one color channel\n",
    "    but NOTE: to see the returned image as grayscale\n",
    "    you should call plt.imshow(gray, cmap='gray')\"\"\"\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "def canny(img, low_threshold, high_threshold):\n",
    "    \"\"\"Applies the Canny transform\"\"\"\n",
    "    return cv2.Canny(img, low_threshold, high_threshold)\n",
    "\n",
    "def gaussian_blur(img, kernel_size):\n",
    "    \"\"\"Applies a Gaussian Noise kernel\"\"\"\n",
    "    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
    "\n",
    "def get_ROI(img, vertices):\n",
    "    pass\n",
    "\n",
    "def region_of_interest(img, vertices):\n",
    "    \"\"\"\n",
    "    Applies an image mask.\n",
    "\n",
    "    Only keeps the region of the image defined by the polygon\n",
    "    formed from `vertices`. The rest of the image is set to black.\n",
    "\n",
    "    Args:\n",
    "      img:\n",
    "      vertices:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    #defining a blank mask to start with\n",
    "    mask = np.zeros_like(img)\n",
    "\n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "\n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color\n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "\n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "\n",
    "def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n",
    "    \"\"\"\n",
    "    `img` should be the output of a Canny transform.\n",
    "\n",
    "    Returns\n",
    "      line_img: Image with hough lines drawn.\n",
    "      lines: Hough lines from the transform of form x1,y1,x2,y2.\n",
    "    \"\"\"\n",
    "    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n",
    "    return lines\n",
    "\n",
    "def weighted_img(img, initial_img, α=0.8, β=1., λ=0.):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      img: Output of the hough_lines(), An image with lines drawn on it.\n",
    "           Should be a blank image (all black) with lines drawn on it.\n",
    "      initial_img: image before any processing.\n",
    "      α: TBD\n",
    "      β: TBD\n",
    "      λ: TBD\n",
    "\n",
    "    The result image is computed as follows:\n",
    "\n",
    "    initial_img * α + img * β + λ\n",
    "    NOTE: initial_img and img must be the same shape!\n",
    "    \"\"\"\n",
    "    return cv2.addWeighted(initial_img, α, img, β, λ)\n",
    "\n",
    "\n",
    "\"\"\"Helper Functions\"\"\"\n",
    "\n",
    "\n",
    "def make_grayscale(x):\n",
    "    x_shape = x.shape\n",
    "    print(x_shape)\n",
    "    num_el = x_shape[0]\n",
    "    ret_images = np.ones((x_shape[0],x_shape[1],x_shape[2]))\n",
    "    print(ret_images.shape)\n",
    "    for i in range(num_el):\n",
    "        curr_im = x[i][:][:][:]\n",
    "        ret_images[i][:][:] = grayscale(curr_im)\n",
    "    return ret_images\n",
    "\n",
    "# def make_grayscale(x):\n",
    "#     x_shape = x.shape\n",
    "#     num_el = x_shape[0]\n",
    "#     ret_images = np.ones((x_shape[0],x_shape[1],x_shape[2]))\n",
    "#     for i in range(num_el):\n",
    "#         curr_im = x[i][:][:][:]\n",
    "#         r = curr_im[:,:,0]\n",
    "#         b = curr_im[:,:,1]\n",
    "#         g = curr_im[:,:,2]\n",
    "#         curr_im_gray = (r) / 3\n",
    "#         ret_images[i][:][:] = curr_im_gray\n",
    "#     return ret_images\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# \"\"\"Helper functions for data categorization, preprocessing, and exploration\"\"\"\n",
    "# def make_class_dict(y):\n",
    "#     class_dict = {}\n",
    "#     num_el = len(y)\n",
    "#     for i in range(num_el):\n",
    "#         curr_class = y[i]\n",
    "#         if curr_class not in class_dict.keys():\n",
    "#             class_dict[curr_class] = [i]\n",
    "#         else:\n",
    "#             pos_index = class_dict[curr_class]\n",
    "#             pos_index.append(i)\n",
    "#             class_dict[curr_class] = pos_index\n",
    "#     return class_dict\n",
    "\n",
    "# def plot_random(X, class_dict):\n",
    "#     for curr_class in class_dict.keys():\n",
    "#         pos_index = class_dict[curr_class]\n",
    "#         len_index = len(pos_index)\n",
    "#         i1 = random.randrange(len_index)\n",
    "#         i2 = random.randrange(len_index)\n",
    "#         i3 = random.randrange(len_index)\n",
    "#         print('Current class = ' + str(curr_class))\n",
    "#         index1 = pos_index[i1]\n",
    "#         index2 = pos_index[i2]\n",
    "#         index3 = pos_index[i3]\n",
    "#         im1 = X[index1][:][:][:]\n",
    "#         im2 = X[index2][:][:][:]\n",
    "#         im3 = X[index3][:][:][:]\n",
    "#         plt.figure()\n",
    "#         plt.subplot(131)\n",
    "#         plt.imshow(im1, cmap='Greys_r')\n",
    "#         plt.subplot(132)\n",
    "#         plt.imshow(im2, cmap='Greys_r')\n",
    "#         plt.subplot(133)\n",
    "#         plt.imshow(im3, cmap='Greys_r')\n",
    "#         plt.show()\n",
    "#         mean_im1, max_im1 = np.mean(im1), np.max(im1)\n",
    "#         mean_im2, max_im2 = np.mean(im2), np.max(im2)\n",
    "#         mean_im3, max_im3 = np.mean(im3), np.max(im3)\n",
    "#         print('Mean of im1,2,3 = ', mean_im1, mean_im2, mean_im3)\n",
    "#         print('Max of im1,2,3 = ', max_im1, max_im2, max_im3)\n",
    "#     plt.close(\"all\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12630, 32, 32, 3)\n",
      "(12630, 32, 32)\n",
      "(12630, 32, 32)\n",
      "(12630, 32, 32)\n",
      "(12630, 32, 32)\n",
      "(12630, 32, 32)\n",
      "(39209, 32, 32, 3)\n",
      "(39209, 32, 32)\n",
      "(39209, 32, 32)\n",
      "(39209, 32, 32)\n",
      "(39209, 32, 32)\n",
      "(39209, 32, 32)\n",
      "(12630, 32, 32, 3)\n",
      "(12630, 32, 32)\n",
      "(39209, 32, 32, 3)\n",
      "(39209, 32, 32)\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(12630, 32, 32) (12630,)\n",
      "(12630, 32, 32) (12630,)\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(39209, 32, 32) (39209,)\n",
      "(39209, 32, 32) (39209,)\n",
      "Making one hot encoding\n",
      "<class 'numpy.ndarray'> <class 'int'>\n",
      "(12630,) 43\n",
      "12630\n",
      "Return y =  (12630, 43)\n",
      "Making one hot encoding\n",
      "<class 'numpy.ndarray'> <class 'int'>\n",
      "(39209,) 43\n",
      "39209\n",
      "Return y =  (39209, 43)\n",
      "Length is =  3\n",
      "Expanding dataset to [num el, row, col, 1]\n",
      "Length is =  3\n",
      "Expanding dataset to [num el, row, col, 1]\n",
      "(12630, 32, 32) (12630,)\n",
      "(39209, 32, 32) (39209,)\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      " \n",
      "(12630, 43) (39209, 43)\n",
      " \n",
      "Type of training data =  <class 'numpy.ndarray'>\n",
      "Type of training labels =  <class 'numpy.ndarray'>\n",
      "Shape of training labels =  (39209, 43)\n",
      "Shape of training data =  (39209, 32, 32, 1)\n",
      "Length is =  3\n",
      "Expanding dataset to [num el, row, col, 1]\n",
      "Length is =  3\n",
      "Expanding dataset to [num el, row, col, 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# TODO: fill this in based on where you saved the training and testing data\n",
    "training_file = '/Users/blakejacquot/Dropbox/MOOCs/Udacity_SelfDrivingCar/Term1/TrafficSignClassifier/traffic-signs-data/train.p'\n",
    "testing_file = '/Users/blakejacquot/Dropbox/MOOCs/Udacity_SelfDrivingCar/Term1/TrafficSignClassifier/traffic-signs-data/test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "\ttrain = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "\ttest = pickle.load(f)\n",
    "\n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "\n",
    "\"\"\"Preprocess datasets\"\"\"\n",
    "X_test_preproc = X_test\n",
    "X_test_preproc = make_grayscale(X_test_preproc)\n",
    "X_test_preproc = make_gaussian_blur(X_test_preproc, 1)\n",
    "X_test_preproc = normalize(X_test_preproc)\n",
    "\n",
    "X_train_preproc = X_train\n",
    "X_train_preproc = make_grayscale(X_train_preproc)\n",
    "X_train_preproc = make_gaussian_blur(X_train_preproc, 1)\n",
    "X_train_preproc = normalize(X_train_preproc)\n",
    "\n",
    "\"\"\"Format training and test data\"\"\"\n",
    "X_test_gray = make_grayscale(X_test)\n",
    "X_train_gray = make_grayscale(X_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#[X_test_shuff, y_test_shuff] = ml.randomize_set(X_test_gray, y_test)\n",
    "#[X_train_shuff, y_train_shuff] = ml.randomize_set(X_train_gray, y_train)\n",
    "[X_test_shuff, y_test_shuff] = randomize_set(X_test_preproc, y_test)\n",
    "[X_train_shuff, y_train_shuff] = randomize_set(X_train_preproc, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_shuff_onehot_test = make_one_hot_encoding(y_test_shuff, 43)\n",
    "y_shuff_onehot_train = make_one_hot_encoding(y_train_shuff, 43)\n",
    "\n",
    "\n",
    "training_data = expand_x(X_train_shuff)\n",
    "training_labels = y_shuff_onehot_train\n",
    "\n",
    "\n",
    "test_data = expand_x(X_test_shuff)\n",
    "test_labels = y_shuff_onehot_test\n",
    "\n",
    "\n",
    "\"\"\"Print data stats\"\"\"\n",
    "print(X_test_shuff.shape, y_test_shuff.shape)\n",
    "print(X_train_shuff.shape, y_train_shuff.shape)\n",
    "print(type(X_test_shuff), type(y_test_shuff))\n",
    "print(type(X_train_shuff), type(y_train_shuff))\n",
    "print(' ')\n",
    "print(y_shuff_onehot_test.shape, y_shuff_onehot_train.shape)\n",
    "print(' ')\n",
    "print('Type of training data = ', type(training_data))\n",
    "print('Type of training labels = ', type(training_labels))\n",
    "print('Shape of training labels = ', training_labels.shape)\n",
    "print('Shape of training data = ', training_data.shape)\n",
    "\n",
    "\n",
    "#sys.exit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length is =  3\n",
      "Expanding dataset to [num el, row, col, 1]\n",
      "Length is =  3\n",
      "Expanding dataset to [num el, row, col, 1]\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "training_epochs = 300\n",
    "\n",
    "n_input = 1024  # Data input taps. 32 * 32 = 1024\n",
    "n_classes = 43  # Total classes\n",
    "\n",
    "layer_width = {\n",
    "\t'layer_1': 32,\n",
    "\t'layer_2': 64,\n",
    "\t'layer_3': 128,\n",
    "\t'fully_connected': 512\n",
    "}\n",
    "\n",
    "# Set up data\n",
    "training_data = expand_x(X_train_shuff)\n",
    "training_labels = y_shuff_onehot_train\n",
    "total_samples = len(training_labels)\n",
    "\n",
    "test_data = expand_x(X_test_shuff)\n",
    "test_labels = y_shuff_onehot_test\n",
    "\n",
    "\n",
    "#0 input 1 or 3 maps of 48x48 neurons\n",
    "#1 convolutional 100 maps of 46x46 neurons 3x3\n",
    "#2 max pooling 100 maps of 23x23 neurons 2x2\n",
    "\n",
    "#3 convolutional 150 maps of 20x20 neurons 4x4\n",
    "#4 max pooling 150 maps of 10x10 neurons 2x2\n",
    "\n",
    "#5 convolutional 250 maps of 8x8 neurons 3x3\n",
    "#6 max pooling 250 maps of 4x4 neurons 2x2\n",
    "#7 fully connected 200 neurons\n",
    "#8 fully connected 43 neurons\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "\t'layer_1': tf.Variable(tf.truncated_normal(\n",
    "\t\t[3, 3, 1, layer_width['layer_1']])),\n",
    "\t'layer_2': tf.Variable(tf.truncated_normal(\n",
    "\t\t[4, 4, layer_width['layer_1'], layer_width['layer_2']])),\n",
    "\t'layer_3': tf.Variable(tf.truncated_normal(\n",
    "\t\t[3, 3, layer_width['layer_2'], layer_width['layer_3']])),\n",
    "\t'fully_connected': tf.Variable(tf.truncated_normal(\n",
    "\t\t[1024, layer_width['fully_connected']])),\n",
    "\t'out': tf.Variable(tf.truncated_normal(\n",
    "\t\t[layer_width['fully_connected'], n_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "# \tweights = {\n",
    "# \t\t'layer_1': tf.Variable(tf.truncated_normal(\n",
    "# \t\t\t[5, 5, 1, layer_width['layer_1']])),\n",
    "# \t\t'layer_2': tf.Variable(tf.truncated_normal(\n",
    "# \t\t\t[5, 5, layer_width['layer_1'], layer_width['layer_2']])),\n",
    "# \t\t'layer_3': tf.Variable(tf.truncated_normal(\n",
    "# \t\t\t[5, 5, layer_width['layer_2'], layer_width['layer_3']])),\n",
    "# \t\t'fully_connected': tf.Variable(tf.truncated_normal(\n",
    "# \t\t\t[1024, layer_width['fully_connected']])),\n",
    "# \t\t'out': tf.Variable(tf.truncated_normal(\n",
    "# \t\t\t[layer_width['fully_connected'], n_classes]))\n",
    "# \t}\n",
    "\n",
    "biases = {\n",
    "\t'layer_1': tf.Variable(tf.zeros(layer_width['layer_1'])),\n",
    "\t'layer_2': tf.Variable(tf.zeros(layer_width['layer_2'])),\n",
    "\t'layer_3': tf.Variable(tf.zeros(layer_width['layer_3'])),\n",
    "\t'fully_connected': tf.Variable(tf.zeros(layer_width['fully_connected'])),\n",
    "\t'out': tf.Variable(tf.zeros(n_classes))\n",
    "}\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, 32, 32, 1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "logits = conv_net(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n",
    "\t.minimize(cost)\n",
    "\n",
    "\n",
    "# Initializing the variables\n",
    "#init = tf.initialize_all_variables()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "At least two variables have the same name: Variable_45",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-61423299a371>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Object to save and restore variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Launch the graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/blakejacquot/anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder)\u001b[0m\n\u001b[1;32m    859\u001b[0m           \u001b[0mkeep_checkpoint_every_n_hours\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_checkpoint_every_n_hours\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m           restore_sequentially=restore_sequentially)\n\u001b[0m\u001b[1;32m    862\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaverDef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saver_def must if a saver_pb2.SaverDef: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msaver_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/blakejacquot/anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, names_to_variables, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, filename)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0munique\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \"\"\"\n\u001b[0;32m--> 502\u001b[0;31m     \u001b[0mvars_to_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ValidateAndSliceInputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_to_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_to_keep\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m       \u001b[0mmax_to_keep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/blakejacquot/anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_ValidateAndSliceInputs\u001b[0;34m(self, names_to_variables)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \"\"\"\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_to_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m       \u001b[0mnames_to_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VarListToDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_to_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0mvars_to_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/blakejacquot/anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_VarListToDict\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames_to_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m           raise ValueError(\"At least two variables have the same name: %s\" %\n\u001b[0;32m--> 377\u001b[0;31m                            name)\n\u001b[0m\u001b[1;32m    378\u001b[0m         \u001b[0mnames_to_variables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: At least two variables have the same name: Variable_45"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver() # Object to save and restore variables.\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(init)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    # Training cycle\n",
    "    print('Training')\n",
    "    for epoch in range(training_epochs):\n",
    "        start_time = time.time()\n",
    "        total_batch = int(total_samples/batch_size)\n",
    "\n",
    "        # Restore model checkpoint\n",
    "        #new_saver = tf.train.import_meta_graph('model-checkpoint.meta')\n",
    "        #new_saver.restore(sess, 'model-checkpoint.ckpt')\n",
    "        #all_vars = tf.trainable_variables()\n",
    "        #for v in all_vars:\n",
    "        #    print(v.name)\n",
    "        \n",
    "           \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x = training_data[i*batch_size:i*batch_size+batch_size,:,:]\n",
    "            batch_y = training_labels[i*batch_size:i*batch_size+batch_size]\n",
    "\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        c = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "        # Save model checkpoint\n",
    "        saver.save(sess, 'model-checkpoint')\n",
    "        # `save` method will call `export_meta_graph` implicitly.\n",
    "        # you will get 2 saved files:my-model.ckpt and my-model.ckpt.meta\n",
    "\n",
    "        \n",
    "        # Print info about current epoch\n",
    "        print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c))\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('Time for epoch (sec) = ', int(elapsed_time))\n",
    "        print(' ')\n",
    "\n",
    "        \n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\n",
    "        \"Accuracy:\",\n",
    "        accuracy.eval({x: test_data, y: test_labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
