# Flow control for statistics
save_stats_to_file = 0
load_stats_from_file = 0
report_stats_from_file = 0
initialize_stat_file = 0

# Flow control for training
restore_model_for_continued_work = 0
train_model = 0
check_model = 0


"""Import modules"""
import jupyter
import numpy as np
import cv2
import matplotlib.pyplot as plt
import scipy
import sklearn
import tensorflow as tf
import math
import random
import time
import math
import random
import pickle
import os

"""Define helper functions for pre-processing"""
def make_gaussian_blur(x, kernel_size):
    print('Making gaussian blur')
    x_shape = x.shape
    #print(x_shape)
    num_el = x_shape[0]
    ret_images = np.ones((x_shape[0],x_shape[1],x_shape[2]))
    #print(ret_images.shape)
    for i in range(num_el):
        curr_im = x[i][:][:][:]
        ret_images[i][:][:] = gaussian_blur(curr_im, kernel_size)
    return ret_images

def crop_to_ROI(x, vertices):
    x_shape = x.shape
    #print(x_shape)
    num_el = x_shape[0]
    ret_images = np.ones((x_shape[0],x_shape[1],x_shape[2]))
    #print(ret_images.shape)
    for i in range(num_el):
        curr_im = x[i][:][:][:]
        ret_images[i][:][:] = get_ROI(curr_im, vertices)
    return ret_images

def normalize(x):
    print('Normalizing data')
    x_shape = x.shape
    #print(x_shape)
    num_el = x_shape[0]
    ret_images = np.ones((x_shape[0],x_shape[1],x_shape[2]))
    #print(ret_images.shape)
    for i in range(num_el):
        curr_im = x[i][:][:][:]
        empty_im = np.ones((x_shape[1],x_shape[2]))
        proc_im = cv2.normalize(curr_im, empty_im, -127,128,cv2.NORM_MINMAX)
        ret_images[i][:][:] = proc_im
    return ret_images

def grayscale(img):
    """Applies the Grayscale transform
    This will return an image with only one color channel
    but NOTE: to see the returned image as grayscale
    you should call plt.imshow(gray, cmap='gray')"""
    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

def gaussian_blur(img, kernel_size):
    """Applies a Gaussian Noise kernel"""
    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)

def make_grayscale(x):
    x_shape = x.shape
    #print(x_shape)
    num_el = x_shape[0]
    ret_images = np.ones((x_shape[0],x_shape[1],x_shape[2]))
    #print(ret_images.shape)
    for i in range(num_el):
        curr_im = x[i][:][:][:]
        ret_images[i][:][:] = grayscale(curr_im)
    return ret_images

def randomize_set(x,y):
    numel = len(y)
    #print(type(x), type(y))
    #print(x.shape, y.shape)
    listicle = [[i] for i in range(numel)]
    random.shuffle(listicle)
    x_shape = x.shape
    y_shape = y.shape
    ret_x = np.ones((x_shape[0],x_shape[1],x_shape[2]))
    ret_y = np.ones((x_shape[0]))
    #print(ret_x.shape, ret_y.shape)
    for i in range(numel):
        index = listicle[i]
        curr_x = x[index,:,:]
        curr_y = y[index]
        ret_x[i,:,:] = curr_x
        ret_y[i] = curr_y
        #print(index)
    return(ret_x,ret_y)

def make_one_hot_encoding(y, num_labels):
    print('Making one hot encoding')
    y_shape = y.shape
    numel = y_shape[0]
    ret_y = np.zeros((numel, num_labels))
    for i in range(numel):
        curr_label = y[i]
        #print('Current label = ', curr_label)
        curr_encoding = np.zeros(num_labels)
        for j in range(num_labels):
            if j == int(curr_label):
                #print('Match!', j, curr_label)
                curr_encoding[j] = 1.0
        #print('Print one-hot encoding of label = ', curr_encoding)
        ret_y[i] = curr_encoding
    return ret_y

def expand_x(x):
    shape_x = x.shape
    #print('Length is = ', len(shape_x))
    if len(shape_x) == 3:
        print('Expanding dataset to [num el, row, col, 1]')
        ret_x = np.empty((shape_x[0],shape_x[1],shape_x[2],1))
        ret_x[:,:,:,0] = x
        #print(ret_x.shape)
        #print('Example value = ', ret_x[0,0,0,0])
    return(ret_x)



"""Load and preprocess data"""
training_file = '/Users/blakejacquot/Dropbox/MOOCs/Udacity_SelfDrivingCar/Term1/TrafficSignClassifier/traffic-signs-data/train.p'
testing_file = '/Users/blakejacquot/Dropbox/MOOCs/Udacity_SelfDrivingCar/Term1/TrafficSignClassifier/traffic-signs-data/test.p'

with open(training_file, mode='rb') as f:
    train = pickle.load(f)
    with open(testing_file, mode='rb') as f:
        test = pickle.load(f)

X_train, y_train = train['features'], train['labels']
X_test, y_test = test['features'], test['labels']

"""Preprocess datasets"""
X_test_preproc = X_test
X_test_preproc = make_grayscale(X_test_preproc)
X_test_preproc = make_gaussian_blur(X_test_preproc, 1)
X_test_preproc = normalize(X_test_preproc)
[X_test_shuff, y_test_shuff] = randomize_set(X_test_preproc, y_test)
y_shuff_onehot_test = make_one_hot_encoding(y_test_shuff, 43)
test_data = expand_x(X_test_shuff)
test_labels = y_shuff_onehot_test

X_train_preproc = X_train
X_train_preproc = make_grayscale(X_train_preproc)
X_train_preproc = make_gaussian_blur(X_train_preproc, 1)
X_train_preproc = normalize(X_train_preproc)
[X_train_shuff, y_train_shuff] = randomize_set(X_train_preproc, y_train)
y_shuff_onehot_train = make_one_hot_encoding(y_train_shuff, 43)
training_data = expand_x(X_train_shuff)
training_labels = y_shuff_onehot_train
total_samples = len(training_labels)

"""Print data info"""
print(' ')
print('Data info')
print('Shape of training labels = ', training_labels.shape)
print('Shape of training data = ', training_data.shape)
print('Shape of test labels = ', test_labels.shape)
print('Shape of test data = ', test_data.shape)


"""Examine One-hot encoding"""
i = 11
print(y_train_shuff[i],y_shuff_onehot_train[i])
print(y_test_shuff[i],y_shuff_onehot_test[i])



"""Examine pre-processed data"""

print('Plotting images and classes from randomized test set')
numel = len(test_labels)
for i in range(3):
    curr_index = random.randrange(numel)
    curr_im = test_data[curr_index,:,:,0]
    curr_class_int = y_test_shuff[curr_index]
    curr_class_onehot = y_shuff_onehot_test[curr_index]

    print('Current index = ' + str(curr_index))
    print('Current image shape = ', curr_im.shape)
    print('Current class in decimal = ' + str(curr_class_int))
    print('Current class one hot encoded = ' + str(curr_class_onehot))

    #plt.figure()
    #plt.imshow(curr_im)
    #plt.show()
    #print(' ')
plt.close("all")

print('************************************************')

print('Plotting images and classes from randomized training set')
numel = len(training_labels)
for i in range(3):
    curr_index = random.randrange(numel)
    curr_im = training_data[curr_index,:,:,0]
    curr_class_int = y_train_shuff[curr_index]
    curr_class_onehot = y_shuff_onehot_train[curr_index]
    print('Current index = ' + str(curr_index))
    print('Current image shape = ', curr_im.shape)
    print('Current class in decimal = ' + str(curr_class_int))
    print('Current class one hot encoded = ' + str(curr_class_onehot))
    #plt.figure()
    #plt.imshow(curr_im)
    #print(' ')
    #plt.show()

plt.close("all")





"""Helper functions for machine learning"""
def conv2d(x, W, b, strides=1):
    """
    Args:
        x
        W
        b
        strides
    Returns:
        TBD
    """
    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')
    x = tf.nn.bias_add(x, b)
    return tf.nn.tanh(x)

def maxpool2d(x, k=2):
    """
    Args:
        x
        k
    Returns:
        TBD
    """
    return tf.nn.max_pool(
        x,
        ksize=[1, k, k, 1],
        strides=[1, k, k, 1],
        padding='SAME')

# Create model
def conv_net(x, weights, biases):
    """Make a model for the network
    Args:
        x:
        weights:
        biases:

    Returns:
        out:

    """

    # Layer 1
    conv1 = conv2d(x, weights['layer_1'], biases['layer_1'])
    conv1 = maxpool2d(conv1, k=2)

    # Layer 2
    conv2 = conv2d(conv1, weights['layer_2'], biases['layer_2'])
    conv2 = maxpool2d(conv2, k=2)

    # Layer 3
    conv3 = conv2d(conv2, weights['layer_3'], biases['layer_3'])
    conv3 = maxpool2d(conv2, k=2)

    # Fully connected layer
    # Reshape conv3 output to fit fully connected layer input
    val = weights['fully_connected'].get_shape().as_list()[0]
    print('Val = ', val)
    print('Weights fully connected = ', weights['fully_connected'])
    print('Biases fully connected = ', biases['fully_connected'])
    fc1 = tf.reshape(
        conv3,
        [-1, weights['fully_connected'].get_shape().as_list()[0]])
    fc1 = tf.add(
        tf.matmul(fc1, weights['fully_connected']),
        biases['fully_connected'])
    fc1 = tf.nn.tanh(fc1)

    # Output Layer - class prediction
    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])
    return out




"""Set model parameters"""
#learning_rate = 0.001
learning_rate = 0.01
batch_size = 128
training_epochs = 200
n_input = 1024  # Data input taps. 32 * 32 = 1024
n_classes = 43  # Total classes

layer_width = {
    'layer_1': 32,
    'layer_2': 64,
    'layer_3': 128,
    'fully_connected': 512
    }

# Store layers weight & bias
weights = {
    # 5x5 conv, 1 input, layer_width['layer_1'] outputs
    'layer_1': tf.Variable(tf.random_normal(
        [5, 5, 1, layer_width['layer_1']])),

    # 5x5 conv, layer_width['layer_1'] inputs, layer_width['layer_2'] outputs
    'layer_2': tf.Variable(tf.random_normal(
        [5, 5, layer_width['layer_1'], layer_width['layer_2']])),

    # 5x5 conv, layer_width['layer_2'] inputs, layer_width['layer_3'] outputs
    'layer_3': tf.Variable(tf.random_normal(
        [5, 5, layer_width['layer_2'], layer_width['layer_3']])),

    # fully connected, 1024 inputs, layer_width['fully_connected'] outputs
    'fully_connected': tf.Variable(tf.random_normal(
        [1024, layer_width['fully_connected']])),

    # output, layer_width['fully_connected'] inputs, n_classes outputs
    'out': tf.Variable(tf.truncated_normal(
        [layer_width['fully_connected'], n_classes]))
}

print('Layer 1 output taps = ', layer_width['layer_1'])
print('Layer 2 output taps = ', layer_width['layer_2'])
print('Layer 3 output taps = ', layer_width['layer_3'])

biases = {
    'layer_1': tf.Variable(tf.zeros(layer_width['layer_1'])),
    'layer_2': tf.Variable(tf.zeros(layer_width['layer_2'])),
    'layer_3': tf.Variable(tf.zeros(layer_width['layer_3'])),
    'fully_connected': tf.Variable(tf.zeros(layer_width['fully_connected'])),
    'out': tf.Variable(tf.zeros(n_classes))
}

# tf Graph input
x = tf.placeholder("float", [None, 32, 32, 1])
y = tf.placeholder("float", [None, n_classes])

# Construct the model
logits = conv_net(x, weights, biases)

# Define loss and optimizer
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y))
optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\
        .minimize(cost)

# Evaluate the model
correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))

# Initializing the variables
init = tf.initialize_all_variables()

saver = tf.train.Saver() # Object to save and restore variables.

"""Flow control used to save out intermediate states"""

# Initialize statistics variables
time_per_training_epoch = []
training_accuracy = []
cost_list = []

"""Initialize or load pickled file containing status data on training"""

if initialize_stat_file == 1:
    print('Saving stats to file')
    data_to_save = {'time_per_training_epoch': time_per_training_epoch, 'training_accuracy': training_accuracy, 'cost_list': cost_list}
    pickle.dump(data_to_save, open( "save.p", "wb" ))

if load_stats_from_file == 1:
    print('Loading stats from file')
    data_to_save = pickle.load(open( "save.p", "rb" ))
    time_per_training_epoch = data_to_save['time_per_training_epoch']
    training_accuracy = data_to_save['training_accuracy']
    cost_list = data_to_save['cost_list']


# Launch the graph
with tf.Session() as sess:
    sess.run(init)

    """Restore saved model for continued work"""
    if restore_model_for_continued_work == 1:
        print('Restoring model')
        start_time_restore = time.time()
        saver = tf.train.import_meta_graph('model-checkpoint.meta')
        saver.restore(sess, 'model-checkpoint')
        all_vars = tf.trainable_variables()
        elapsed_time = time.time() - start_time_restore
        print('Time to restore model (sec) = ', int(elapsed_time))

    """Train model"""
    if train_model == 1:
        print('Training model')
        for epoch in range(training_epochs):
            print('Starting epoch = ', epoch)
            start_time_train = time.time()
            total_batch = int(total_samples/batch_size)

            # Loop over all batches
            print('Processing batches. Not yet saving.')
            for i in range(total_batch):
                start = i*batch_size
                stop = i*batch_size+batch_size
                batch_x = training_data[start:stop,:,:]
                batch_y = training_labels[start:stop]

                # Run optimization op (backprop) and cost op (to get loss value)
                sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})

            # Display logs per epoch step
            c = sess.run(cost, feed_dict={x: batch_x, y: batch_y})

            print(type(c))

            cost_list.append(c)

            # Save model checkpoint
            print('Saving session')
            saver.save(sess, 'model-checkpoint')

            # Print info about the epoch
            elapsed_time = time.time() - start_time_train
            print('Time to process epoch (sec) = ', int(elapsed_time))
            time_per_training_epoch.append(int(elapsed_time))
            print(' ')
            print("Epoch:", '%04d' % (epoch+1), "cost=", "{:.9f}".format(c))

            """Test model and report accuracy"""
            print('Calculating Accuracy')
            start_time_test_report = time.time()
            ################################################################################
            ###############################################################################
            ################################################################################
            # Previously, 'logits' was defined as: logits = conv_net(x, weights, biases)
            # tf.argmax(logits, 1) is ...
            # tf.argmax(y, 1) is ...
            # tf.equal(x,y) the truth value of (x == y) element-wise.
            # correct_prediction is tensor of type 'bool'
            # tf.reduce_mean(input_tensor) computes the mean of elements across dimensions of a tensor


            # Calculate accuracy for test images
            #accuracy_val = accuracy.eval({x: test_data, y: test_labels})
            accuracy_val = sess.run(accuracy, feed_dict={x: test_data,
                                      y: test_labels
                                      })
            print("Accuracy:", accuracy_val)
            ################################################################################
            ################################################################################
            ################################################################################


            elapsed_time = time.time() - start_time_test_report
            training_accuracy.append(accuracy_val)
            print('Time to test and report accuracy (sec) = ', int(elapsed_time))

            if save_stats_to_file == 1:
                print('Saving stats to file')
                data_to_save = {'time_per_training_epoch': time_per_training_epoch, 'training_accuracy': training_accuracy, 'cost_list': cost_list}
                pickle.dump(data_to_save, open( "save.p", "wb" ))

            if report_stats_from_file == 1:
                print('Reporting from file to make plot')
                data_to_save = pickle.load( open( "save.p", "rb" ) )
                time_per_training_epoch = data_to_save['time_per_training_epoch']
                training_accuracy = data_to_save['training_accuracy']

                """Plot and save fig for time per training epoch"""
                fig_savename = 'time_per_training_epoch.png'
                fig = plt.figure()
                curr_plot1 = plt.plot(range(len(time_per_training_epoch)), time_per_training_epoch, color = 'b')
                plt.ylabel('Time per training epoch (sec)')
                plt.title('Training time per epoch', fontsize = 10)
                curr_dir = os.getcwd()
                fig.savefig(fig_savename)
                fig.clf()

                """Plot cost"""
                fig_savename = 'cost.png'
                fig = plt.figure()
                curr_plot1 = plt.plot(range(len(cost_list)), cost_list, color = 'b')
                plt.ylabel('Cost')
                plt.title('Cost', fontsize = 10)
                curr_dir = os.getcwd()
                fig.savefig(fig_savename)
                fig.clf()

                """Plot and save fig for model accuracy"""
                fig_savename = 'training_accuracy.png'
                fig = plt.figure()
                curr_plot1 = plt.plot(range(len(training_accuracy)), training_accuracy, color = 'b')
                plt.ylabel('Training accuracy')
                plt.title('Training accuracy', fontsize = 10)
                curr_dir = os.getcwd()
                fig.savefig(fig_savename)
                fig.clf()


                print('Done reporting stats')
                plt.close("all")


        print("Optimization Finished!")
































"""Check model"""
if check_model == 1:
    print('Checking the model')
    with tf.Session() as sess:
        sess.run(init)

        # Restore model
        if restore_model_for_continued_work == 1:
            print('Restoring model')
            start_time_restore = time.time()
            saver = tf.train.import_meta_graph('model-checkpoint.meta')
            saver.restore(sess, 'model-checkpoint')
            all_vars = tf.trainable_variables()
            elapsed_time = time.time() - start_time_restore
            print('Time to restore model (sec) = ', int(elapsed_time))


## Construct the model
#logits = conv_net(x, weights, biases)

## Define loss and optimizer
#cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y))
#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\
#        .minimize(cost)

## Evaluate the model



        # Get image and label
        print(type(test_data))
        print(type(test_labels))
        print(test_data.shape, test_labels.shape)
        temp_data = test_data[0:1,:,:,:]
        temp_labels = test_labels[0:1,:]

        # See if prediction is correct
        most_likely_label_for_input = tf.argmax(logits, 1)
        actual_label = tf.argmax(y, 1)
        print('Most likely label = ', most_likely_label_for_input)
        print('Actual label = ', actual_label)
        correct_prediction = tf.equal(most_likely_label_for_input, actual_label) # Returns list of Booleans.

        # Determine what fraction of predictions are correct
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # returns reduced tensor

        print(type(correct_prediction))
        print(type(accuracy))
        #accuracy_val = sess.run(accuracy, feed_dict={x: temp_data, y: temp_labels})
        accuracy_val = accuracy.eval({x: temp_data, y: temp_labels})
        print(type(accuracy_val))
        print("Accuracy:", accuracy_val)



        # Infer label from image

        # Compare inferred label to actual label
